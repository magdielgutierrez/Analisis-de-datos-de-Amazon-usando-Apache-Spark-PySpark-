{"cells": [{"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nspark = SparkSession.builder \\\n  .appName('clean_products') \\\n  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "##extract table compras from BigQuery Staging ######"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- client_id: string (nullable = true)\n |-- product_price: double (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_quantity: long (nullable = true)\n |-- purchase_date: date (nullable = true)\n\nlines incoming:  1965807\n"}], "source": "#name table compras\ntable_compras = \"becade_mgutierrez.pr_compras\"\n\n#load table\nraw_compras = spark.read \\\n  .format(\"bigquery\") \\\n  .option(\"table\", table_compras) \\\n  .load()\n\n#show schema\nraw_compras.printSchema()\n\n#show incoming lines\nprint(\"lines incoming: \" , raw_compras.count())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------+----------+----------------+-------------+-----------+----------+\n|        client_id|product_price|product_id|product_quantity|purchase_date|month_sales|year_sales|\n+-----------------+-------------+----------+----------------+-------------+-----------+----------+\n|209-696678-32-117|       473.98|B00N69D6AS|               2|   2012-03-16|          3|      2012|\n|209-696678-32-117|       473.98|B00N69D6AS|               2|   2013-03-22|          3|      2013|\n+-----------------+-------------+----------+----------------+-------------+-----------+----------+\nonly showing top 2 rows\n\nlines source:  1965807\n"}], "source": "from pyspark.sql.functions  import year, month\n\ndf_new_sales= raw_compras.withColumn('month_sales',month(raw_compras.purchase_date)) \\\n                .withColumn('year_sales',year(raw_compras.purchase_date))\n\ndf_new_sales.show(2)\n#df_new_sales.printSchema()\nprint(\"lines source: \" , raw_compras.count())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "############ YEAR ######################"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------------+-----------------+-------------+\n|year_sales|purchase_date|        client_id|total_compras|\n+----------+-------------+-----------------+-------------+\n|      2010|   2010-01-01|602-878245-53-323|            1|\n|      2010|   2010-01-01|661-717208-27-804|            1|\n|      2010|   2010-01-01|408-329226-28-494|            1|\n|      2010|   2010-01-01|831-960523-53-330|            1|\n|      2010|   2010-01-01|805-787952-37-812|            1|\n+----------+-------------+-----------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.functions  import countDistinct\n\n###compras por A\u00f1o\ndf_ordenes_year = df_new_sales.select('year_sales','purchase_date','month_sales','client_id') \\\n        .groupBy('year_sales','purchase_date','client_id') \\\n        .agg(countDistinct('client_id').alias('total_compras')) \\\n        .sort(['year_sales', 'purchase_date'], ascending=True)\n\n#df_year= df_year.withColumn('venta_total', df_year.venta_total.cast(DecimalType(18, 2)))\n\ndf_ordenes_year.show(5)\n# df_ordenes_year.printSchema() "}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------------+\n|year_sales|total_compras|\n+----------+-------------+\n|      2010|        32717|\n|      2011|        32746|\n|      2012|        32899|\n|      2013|        32733|\n|      2014|        32713|\n|      2015|        32957|\n|      2016|        32854|\n|      2017|        32753|\n|      2018|        32724|\n|      2019|        32755|\n|      2020|        32913|\n+----------+-------------+\n\n"}], "source": "from pyspark.sql.functions  import count\n\nsum_ordenes_year = df_ordenes_year.select('year_sales','total_compras') \\\n        .groupBy('year_sales') \\\n        .agg(count('total_compras').alias('total_compras')) \\\n        .sort('year_sales', ascending=True)\nsum_ordenes_year.show()"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+------------------+\n|year_sales|    venta_total_year| avg_venta_mensual|\n+----------+--------------------+------------------+\n|      2010|1.8495069750010703E7|104.32923661415366|\n|      2011|1.8595521430009063E7|104.14100184255835|\n|      2012| 1.891395686000886E7|104.80852955197692|\n|      2013| 1.855743942000939E7|103.84979669274121|\n|      2014|1.8657688250007752E7| 105.1688391664802|\n|      2015| 1.872384307000746E7|104.68142492945773|\n|      2016| 1.867335246000713E7|104.62258287908166|\n|      2017| 1.872034121000714E7|104.20509554746833|\n|      2018|1.8488971880010866E7|104.16204820233499|\n|      2019|1.8643064240007747E7|104.42012243827817|\n|      2020|1.8892210430008642E7|104.74258421676042|\n+----------+--------------------+------------------+\n\n"}], "source": "from pyspark.sql.functions  import sum,avg\n\ndf_sales = df_new_sales.select('year_sales','month_sales','product_price','client_id') \\\n        .groupBy('year_sales') \\\n        .agg(sum('product_price').alias('venta_total_year'), \\\n             avg('product_price').alias('avg_venta_mensual')) \\\n         .sort('year_sales', ascending=True)\n            \n\n#df_year= df_year.withColumn('venta_total', df_year.venta_total.cast(DecimalType(18, 2)))\n\ndf_sales.show()\n#df_year.printSchema() "}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+------------------+-------------+\n|year_sales|    venta_total_year| avg_venta_mensual|total_compras|\n+----------+--------------------+------------------+-------------+\n|      2010|1.8495069750010703E7|104.32923661415366|        32717|\n|      2011|1.8595521430009063E7|104.14100184255835|        32746|\n|      2012| 1.891395686000886E7|104.80852955197692|        32899|\n|      2013| 1.855743942000939E7|103.84979669274121|        32733|\n|      2014|1.8657688250007752E7| 105.1688391664802|        32713|\n|      2015| 1.872384307000746E7|104.68142492945773|        32957|\n|      2016| 1.867335246000713E7|104.62258287908166|        32854|\n|      2017| 1.872034121000714E7|104.20509554746833|        32753|\n|      2018|1.8488971880010866E7|104.16204820233499|        32724|\n|      2019|1.8643064240007747E7|104.42012243827817|        32755|\n|      2020|1.8892210430008642E7|104.74258421676042|        32913|\n+----------+--------------------+------------------+-------------+\n\n"}], "source": "from pyspark.sql.functions  import col\n\n#InnerJoin\nfull_table_year = df_sales.alias('A').join(sum_ordenes_year.alias('B'), col('A.year_sales') == col('B.year_sales'), \"inner\") \n\n#Show first 20 rows\nfull_table_year= full_table_year.select('A.year_sales','A.venta_total_year','A.avg_venta_mensual','B.total_compras') \\\n                 .sort('A.year_sales', ascending=True)\n\nfull_table_year.show()\n"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "########Insert table pr_compras_anuales  to BigQuery Production ###############\n#################################################################################"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": "full_table_year.write \\\n  .format(\"bigquery\") \\\n  .option(\"table\",\"becade_mgutierrez.pr_compras_anuales\") \\\n  .option(\"temporaryGcsBucket\", \"amazon_magdielgutierrez\") \\\n  .mode('overwrite') \\\n  .save()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [], "source": "################ MONTH ####################"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-----------+-------------+-----------------+-------------+\n|year_sales|month_sales|purchase_date|        client_id|total_compras|\n+----------+-----------+-------------+-----------------+-------------+\n|      2010|          1|   2010-01-01|213-242594-75-886|            1|\n|      2010|          1|   2010-01-01|323-079732-31-237|            1|\n|      2010|          1|   2010-01-01|928-662647-10-781|            1|\n|      2010|          1|   2010-01-01|559-163720-78-708|            1|\n|      2010|          1|   2010-01-01|480-970965-40-063|            1|\n+----------+-----------+-------------+-----------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.functions  import countDistinct\n#### datafrem  df_new_sales\n\n###compras por Mes\ndf_ordenes_month = df_new_sales.select('year_sales','purchase_date','month_sales','client_id') \\\n        .groupBy('year_sales','month_sales','purchase_date','client_id') \\\n        .agg(countDistinct('client_id').alias('total_compras')) \\\n        .sort(['year_sales', 'purchase_date'], ascending=True)\n\n#df_year= df_year.withColumn('venta_total', df_year.venta_total.cast(DecimalType(18, 2)))\n\ndf_ordenes_month.show(5)\n# df_ordenes_year.printSchema() "}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "lines source:  132\n+----------+-----------+-----------------+\n|year_sales|month_sales|total_compras_mes|\n+----------+-----------+-----------------+\n|      2010|          1|             2618|\n|      2010|          2|             2406|\n|      2010|          3|             2478|\n|      2010|          4|             2778|\n|      2010|          5|             2786|\n|      2010|          6|             2879|\n|      2010|          7|             2958|\n|      2010|          8|             2889|\n|      2010|          9|             2679|\n|      2010|         10|             2697|\n|      2010|         11|             2580|\n|      2010|         12|             2969|\n|      2011|          1|             2586|\n+----------+-----------+-----------------+\nonly showing top 13 rows\n\n"}], "source": "from pyspark.sql.functions  import count\n\nsum_ordenes_month = df_ordenes_month.select('year_sales','month_sales','total_compras') \\\n        .groupBy('year_sales','month_sales') \\\n        .agg(count('total_compras').alias('total_compras_mes')) \\\n        .sort(['year_sales','month_sales'], ascending=True)\n\nprint(\"lines source: \" , sum_ordenes_month.count())\nsum_ordenes_month.show(13)"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "lines source:  132\n+----------+-----------+------------------+\n|year_sales|month_sales|   venta_total_mes|\n+----------+-----------+------------------+\n|      2010|          1|1241494.2399999024|\n|      2010|          2|1230857.7399999062|\n|      2010|          3| 1118628.349999931|\n|      2010|          4|  1628756.23999979|\n|      2010|          5|1518457.3699998613|\n|      2010|          6|1991022.5199997695|\n|      2010|          7| 1960883.829999772|\n|      2010|          8|1668432.4099997922|\n|      2010|          9|1476454.5499998732|\n|      2010|         10|1468640.0599998764|\n|      2010|         11| 1271374.029999895|\n|      2010|         12|1920068.4099997666|\n|      2011|          1|1220903.2599999004|\n+----------+-----------+------------------+\nonly showing top 13 rows\n\n"}], "source": "from pyspark.sql.functions  import sum,avg\n\ndf_month = df_new_sales.select('year_sales','month_sales','product_price') \\\n        .groupBy('year_sales','month_sales') \\\n        .agg(sum('product_price').alias('venta_total_mes')) \\\n        .sort(['year_sales','month_sales'], ascending=True)\n\n#df_year= df_year.withColumn('venta_total', df_year.venta_total.cast(DecimalType(18, 2)))\nprint(\"lines source: \" , df_month.count())\ndf_month.show(13)\n#df_year.printSchema() "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "lines source:  132\n"}], "source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag\n\ndf_month_raw = df_month.withColumn('venta_total_mes_anterior',lag(df_month['venta_total_mes']).over(Window.orderBy(\"month_sales\",\"year_sales\")))\nprint(\"lines source: \" , df_month_raw.count())\n"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-----------+------------------+------------------------+\n|year_sales|month_sales|   venta_total_mes|venta_total_mes_anterior|\n+----------+-----------+------------------+------------------------+\n|      2010|          1|1241494.2399999024|                     0.0|\n|      2011|          1|1220903.2599999004|      1241494.2399999024|\n|      2012|          1|1207546.9799999103|      1220903.2599999004|\n|      2013|          1|1229377.7699999062|      1207546.9799999103|\n|      2014|          1|1288639.0199998915|      1229377.7699999062|\n+----------+-----------+------------------+------------------------+\nonly showing top 5 rows\n\n"}], "source": "df_month_raw= df_month_raw.na.fill(value=0,subset=[\"venta_total_mes_anterior\"])\ndf_month_raw.show(5)"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "lines source:  132\n+----------+-----------+------------------+------------------------+-----------------+\n|year_sales|month_sales|   venta_total_mes|venta_total_mes_anterior|total_compras_mes|\n+----------+-----------+------------------+------------------------+-----------------+\n|      2010|          1|1241494.2399999024|                     0.0|             2618|\n|      2011|          1|1220903.2599999004|      1241494.2399999024|             2586|\n|      2012|          1|1207546.9799999103|      1220903.2599999004|             2577|\n|      2013|          1|1229377.7699999062|      1207546.9799999103|             2569|\n|      2014|          1|1288639.0199998915|      1229377.7699999062|             2602|\n|      2015|          1|1270299.0199999036|      1288639.0199998915|             2630|\n|      2016|          1|1345551.9299998868|      1270299.0199999036|             2619|\n|      2017|          1|1145680.2099999334|      1345551.9299998868|             2529|\n|      2018|          1|1198162.2299999034|      1145680.2099999334|             2555|\n|      2019|          1|1221042.8499999032|      1198162.2299999034|             2605|\n|      2020|          1|1253193.3999999021|      1221042.8499999032|             2610|\n|      2010|          2|1230857.7399999062|      1253193.3999999021|             2406|\n|      2011|          2|1168152.5199999078|      1230857.7399999062|             2401|\n+----------+-----------+------------------+------------------------+-----------------+\nonly showing top 13 rows\n\n"}], "source": "#InnerJoin\nfull_table_month = df_month_raw.alias('A').join(sum_ordenes_month.alias('B'), \\\n                (col('A.month_sales') == col('B.month_sales')) & (col('A.year_sales') == col('B.year_sales')) , \"inner\") \n\n#Show first 20 rows\nfull_table_month= full_table_month.select('A.year_sales','A.month_sales','A.venta_total_mes','venta_total_mes_anterior','B.total_compras_mes') \\\n              .sort(['month_sales','year_sales'], ascending=True)\n\nprint(\"lines source: \" , full_table_month.count())\nfull_table_month.show(13)"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": "full_table_month.write \\\n  .format(\"bigquery\") \\\n  .option(\"table\",\"becade_mgutierrez.pr_compras_mensuales\") \\\n  .option(\"temporaryGcsBucket\", \"amazon_magdielgutierrez\") \\\n  .mode('overwrite') \\\n  .save()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}